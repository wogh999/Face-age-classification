{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T05:45:03.315674Z",
     "iopub.status.busy": "2023-06-01T05:45:03.315089Z",
     "iopub.status.idle": "2023-06-01T05:45:03.324014Z",
     "shell.execute_reply": "2023-06-01T05:45:03.322643Z",
     "shell.execute_reply.started": "2023-06-01T05:45:03.315635Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader,SubsetRandomSampler\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "import albumentations.pytorch as A1\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "# torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T05:45:03.327016Z",
     "iopub.status.busy": "2023-06-01T05:45:03.326080Z",
     "iopub.status.idle": "2023-06-01T05:45:03.340988Z",
     "shell.execute_reply": "2023-06-01T05:45:03.340182Z",
     "shell.execute_reply.started": "2023-06-01T05:45:03.326985Z"
    }
   },
   "outputs": [],
   "source": [
    "import random                                      ##random변수 시드 값 고정\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  \n",
    "    torch.backends.cudnn.deterministic = True  \n",
    "    torch.backends.cudnn.benchmark = True  \n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T05:45:03.343109Z",
     "iopub.status.busy": "2023-06-01T05:45:03.342255Z",
     "iopub.status.idle": "2023-06-01T05:45:03.351473Z",
     "shell.execute_reply": "2023-06-01T05:45:03.350374Z",
     "shell.execute_reply.started": "2023-06-01T05:45:03.343079Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   #GPU 쿠다 사용을 위한 준비\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-06-01T05:45:03.353614Z",
     "iopub.status.busy": "2023-06-01T05:45:03.352861Z",
     "iopub.status.idle": "2023-06-01T05:45:03.374281Z",
     "shell.execute_reply": "2023-06-01T05:45:03.373358Z",
     "shell.execute_reply.started": "2023-06-01T05:45:03.353567Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18828\\2577182874.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data.csv'\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m#csv 파일 불러오는 코드\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./testdata.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\LEE\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\LEE\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\LEE\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\LEE\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\LEE\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\LEE\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\LEE\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"encoding_errors\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"strict\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m         )\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\LEE\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 707\u001b[1;33m                 \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    708\u001b[0m             )\n\u001b[0;32m    709\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data.csv'"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('./data.csv')   #csv 파일 불러오는 코드\n",
    "test_df = pd.read_csv('./testdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T05:45:03.411774Z",
     "iopub.status.busy": "2023-06-01T05:45:03.410979Z",
     "iopub.status.idle": "2023-06-01T05:45:03.419844Z",
     "shell.execute_reply": "2023-06-01T05:45:03.418788Z",
     "shell.execute_reply.started": "2023-06-01T05:45:03.411744Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for index, row in train_df.iterrows():\n",
    "    print('Image : {}'.format(row[\"Image\"]))\n",
    "    print('Label : {}'.format(row[\"Label\"]))\n",
    "    print('Age   : {}'.format(row[\"Age\"]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './dataset/{}/{}'\n",
    "train_image=[]\n",
    "for index, row in train_df.iterrows():\n",
    "    image_path = row['Image']\n",
    "    image_label = row['Label']\n",
    "    image = Image.open(path.format(image_label, image_path)).convert('RGB')\n",
    "    train_image.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T05:45:03.431656Z",
     "iopub.status.busy": "2023-06-01T05:45:03.431306Z",
     "iopub.status.idle": "2023-06-01T05:45:03.445721Z",
     "shell.execute_reply": "2023-06-01T05:45:03.444537Z",
     "shell.execute_reply.started": "2023-06-01T05:45:03.431627Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset): \n",
    "    def __init__(self, dataframe, train='train', transform=None):\n",
    "        if train == 'train':\n",
    "            self.image_list = []\n",
    "            self.label_list = []\n",
    "            self.other_list = []\n",
    "            path = './dataset/{}/{}'\n",
    "            for index, row in dataframe.iterrows():\n",
    "                image_path = row['Image']\n",
    "                image_label = row['Label']\n",
    "                image_age = row['Age']\n",
    "                image_gender = row['Gender']\n",
    "                image_race = row['Race']\n",
    "                image = Image.open(path.format(image_label, image_path)).convert('RGB')\n",
    "                if transform != None:\n",
    "                    image=np.array(image) #albumentation 사용하기 위해서 형태 변환\n",
    "                    image = transform(image=image)['image']\n",
    "                self.image_list.append(image)\n",
    "                self.label_list.append(image_label)\n",
    "                self.other_list.append((image_age, image_gender, image_race))\n",
    "        elif train == 'test':\n",
    "            self.image_list = []\n",
    "            self.label_list = [] # 이미지의 경로\n",
    "            self.other_list = []\n",
    "            path = './testset/{}'\n",
    "            for index, row in dataframe.iterrows():\n",
    "                image_path = row['Image']\n",
    "                image_gender = row['Gender']\n",
    "                image_race = row['Race']\n",
    "                image = Image.open(path.format(image_path)).convert('RGB')\n",
    "                if transform != None:\n",
    "                    image=np.array(image)\n",
    "                    image = transform(image=image)['image']\n",
    "                self.image_list.append(image)\n",
    "                self.label_list.append(image_path)\n",
    "                self.other_list.append((image_gender, image_race))\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx=int(idx)\n",
    "        return self.image_list[idx], self.label_list[idx], self.other_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23708\\1706763652.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_transform\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test_df' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset에 대한 Data Loaders 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T05:45:27.460169Z",
     "iopub.status.busy": "2023-06-01T05:45:27.458520Z",
     "iopub.status.idle": "2023-06-01T05:45:27.480743Z",
     "shell.execute_reply": "2023-06-01T05:45:27.479749Z",
     "shell.execute_reply.started": "2023-06-01T05:45:27.460133Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)  \n",
    "        self.conv1_bn = nn.BatchNorm2d(64)\n",
    "        \n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(64)\n",
    "        \n",
    "# ---------   \n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv3_bn = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.conv4_bn = nn.BatchNorm2d(128)\n",
    "        \n",
    "# ---------  \n",
    "        self.conv5 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.conv5_bn = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.conv6 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.conv6_bn = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv7 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.conv7_bn = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv8 = nn.Conv2d(512, 1024, 3, padding=1)\n",
    "        self.conv8_bn = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        self.conv9 = nn.Conv2d(1024, 1024, 3, padding=1)\n",
    "        self.conv9_bn = nn.BatchNorm2d(1024)\n",
    "        \n",
    "# ---------  \n",
    "\n",
    "        self.fc1 = nn.Linear(25600+2,12800)\n",
    "        self.fc1_bn = nn.BatchNorm1d(12800)\n",
    "        \n",
    "        self.fc2 = nn.Linear(12800, 4096)\n",
    "        self.fc2_bn = nn.BatchNorm1d(4096)\n",
    "    \n",
    "        self.fc3 = nn.Linear(4096, 1024)\n",
    "        self.fc3_bn = nn.BatchNorm1d(1024)\n",
    "        \n",
    "        self.fc5 = nn.Linear(1024, 5)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.pool5 = nn.MaxPool2d(5, 5)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        \n",
    "        \n",
    "    def forward(self, x,x_meta=None):    \n",
    "       \n",
    "    \n",
    "        # 3 x 224 x 224\n",
    "        x = self.relu(self.conv1_bn(self.conv1(x)))\n",
    "        \n",
    "        # 64 x 224 x 224\n",
    "        x = self.pool(self.relu(self.conv2_bn(self.conv2(x))))\n",
    "\n",
    "        # 64x 112 x 112\n",
    "        x = self.relu(self.conv3_bn(self.conv3(x)))\n",
    "\n",
    "        # 128 x 112 x 112\n",
    "        x = self.pool(self.relu(self.conv4_bn(self.conv4(x))))\n",
    "        \n",
    "        # 128 x 64 x 64        \n",
    "        x = self.relu(self.conv5_bn(self.conv5(x)))\n",
    "        \n",
    "        # 256 x 64 x 64\n",
    "        \n",
    "        x = self.relu(self.conv6_bn(self.conv6(x)))\n",
    "\n",
    "        # 256 x 64 x 64\n",
    "        x = self.relu(self.conv7_bn(self.conv7(x)))\n",
    "        \n",
    "        # 512 x 64 x 64\n",
    "        x = self.pool(self.relu(self.conv8_bn(self.conv8(x))))\n",
    "\n",
    "        # 1024 x 32 x 32\n",
    "        x = self.relu(self.conv9_bn(self.conv9(x)))\n",
    "        \n",
    "        # 1024 x 32 x 32\n",
    "        x = self.pool5(x)\n",
    "\n",
    "        # 1024 x 5 x 5\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = torch.cat((x,x_meta),dim=1) # 여기서 CNN연산과 meta데이터를 합쳐서 연산을 한다.\n",
    "        x = self.relu(self.fc1_bn(self.fc1(x)))\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2_bn(self.fc2(x)))\n",
    "\n",
    "        x = self.relu(self.fc3_bn(self.fc3(x)))\n",
    "        \n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T05:45:30.764746Z",
     "iopub.status.busy": "2023-06-01T05:45:30.763346Z",
     "iopub.status.idle": "2023-06-01T05:45:37.502300Z",
     "shell.execute_reply": "2023-06-01T05:45:37.500551Z",
     "shell.execute_reply.started": "2023-06-01T05:45:30.764713Z"
    }
   },
   "outputs": [],
   "source": [
    "model = VGG16()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-01T08:42:08.686615Z",
     "iopub.status.idle": "2023-06-01T08:42:08.687263Z",
     "shell.execute_reply": "2023-06-01T08:42:08.687032Z",
     "shell.execute_reply.started": "2023-06-01T08:42:08.687010Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "k=5\n",
    "\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True)\n",
    "labels = np.array(train_df['Label']) #불균형으로 나눠지지 않게 하기 위해 라벨을 사용해서 나눌 때 사용한다.\n",
    "\n",
    "\n",
    "for fold,(train_index,valid_index) in enumerate(skf.split(train_df,labels)):\n",
    "\n",
    "    print(f'Fold {fold+1}/{k}')\n",
    "    \n",
    "    n_epochs = 60\n",
    "\n",
    "    #valid_loss가 가장 낮은 값 저장\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    #train과 valid를 따로 전처리를 하기 위해 새로운 데이터 프레임 생성\n",
    "    train_a=pd.DataFrame(columns=['Image','Label','Age','Gender','Race']) \n",
    "    valid_a=pd.DataFrame(columns=['Image','Label','Age','Gender','Race'])\n",
    "    \n",
    "    \n",
    "    #StratifiedKFold를 이용하여 index를 뽑아 데이터 생성\n",
    "    for i in train_index:\n",
    "        train_a.loc[len(train_a)]=list(train_df.loc[i])\n",
    "    train_dataset=train_a\n",
    "    for i in valid_index:\n",
    "        valid_a.loc[len(valid_a)]=list(train_df.loc[i])\n",
    "    valid_dataset=valid_a\n",
    "    \n",
    "    \n",
    "    #미리 정의한 CUstomDataset함수를 이용하여 이미지 전처리 수행\n",
    "    train_dataset = CustomDataset(train_dataset, train='train', transform=train_transform)\n",
    "    valid_dataset = CustomDataset(valid_dataset, train='train', transform=test_transform)\n",
    "\n",
    "    #epoch마다 loss 저장\n",
    "    train_loss = torch.zeros(n_epochs)\n",
    "    valid_loss = torch.zeros(n_epochs)\n",
    "\n",
    "    #epoch마다 acc저장\n",
    "    train_acc = torch.zeros(n_epochs)\n",
    "    valid_acc = torch.zeros(n_epochs)\n",
    "\n",
    "    #model을 초기화 한 뒤 device가 cuda로 되어있으므로 GPU 사용\n",
    "    model = VGG16()\n",
    "    model.to(device)\n",
    "    \n",
    "    #데이터를 batch_size 별로 불러오기 위한 DataLoader 생성\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    #분류 문제를 해결하기 위한 CrossEntropyLoss사용\n",
    "    #Optimzer는 수렴속도가 빠른 Adam대신 가볍고 더 미세하게 조정하면 효과가 있다고 생각한 SGD를 사용\n",
    "    #scheduler는 CosineAnnealingLR를 사용하 cosine 그래프르 그리면 learning rate가 감소한다.\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(),lr=0.01,momentum=0.9,weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=60, eta_min=0.0001)\n",
    "\n",
    "    \n",
    "    \n",
    "    #기본적인 학습 코드\n",
    "    for e in range(0, n_epochs):\n",
    "        \n",
    "        \n",
    "        model.train()\n",
    "        for data, labels,others in tqdm(train_loader):\n",
    "\n",
    "            data, labels,others[1],others[2]= data.to(device), labels.to(device), others[1].to(device).float().reshape(-1,1),others[2].to(device).float().reshape(-1,1)   #others[1] gender,others[2] race\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(data,torch.cat((others[1],others[2]),dim=1))\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "\n",
    "            train_loss[e] += loss.item()\n",
    "            \n",
    "            #softmax를 사용하여 제일 확률 값의 class를 정답으로 뽑아낸다.\n",
    "            ps = F.softmax(logits, dim=1)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            equals = top_class == labels.reshape(top_class.shape)\n",
    "            train_acc[e] += torch.mean(equals.type(torch.float)).detach().cpu()\n",
    "            \n",
    "\n",
    "        train_loss[e] /= len(train_loader)\n",
    "        train_acc[e] /= len(train_loader)\n",
    "\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            model.eval()\n",
    "            for data, labels, others in tqdm(valid_loader):\n",
    "                data, labels,others[1],others[2]= data.to(device), labels.to(device),others[1].to(device).float().reshape(-1,1),others[2].to(device).float().reshape(-1,1)\n",
    "                logits = model(data,torch.cat((others[1],others[2]),dim=1))\n",
    "                loss = criterion(logits, labels)\n",
    "                valid_loss[e] += loss.item()\n",
    "\n",
    "                ps = F.softmax(logits, dim=1)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.reshape(top_class.shape)\n",
    "                valid_acc[e] += torch.mean(equals.type(torch.float)).detach().cpu()\n",
    "\n",
    "        valid_loss[e] /= len(valid_loader)\n",
    "        valid_acc[e] /= len(valid_loader)\n",
    "        print(\"lr: \", optimizer.param_groups[0]['lr'])\n",
    "        scheduler.step()\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            e, train_loss[e], valid_loss[e]))\n",
    "\n",
    "        print('Epoch: {} \\tTraining accuracy: {:.6f} \\tValidation accuracy: {:.6f}'.format(\n",
    "            e, train_acc[e], valid_acc[e]))\n",
    "\n",
    "        if valid_loss[e] <= valid_loss_min: #valid loss 기준 더 낮은 값으로 저장\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss[e]))\n",
    "            torch.save(model, f'model_{fold+1}.pt') #모델 저장\n",
    "            valid_loss_min = valid_loss[e]\n",
    "    \n",
    "    #모델 별 confusion matrix를 뽑아내는 코드이다.\n",
    "    model=torch.load(f'./model_{fold+1}.pt')\n",
    "    classes=['1~10', '11~20', '21~30', '31~40', '41~']\n",
    "    classes_cm=[0, 1, 2, 3, 4]\n",
    "    test_loss = 0\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    test_acc = 0\n",
    "    with torch.no_grad(): \n",
    "        model.eval()\n",
    "        for data, labels, other in valid_loader:\n",
    "            data, labels,other[1],other[2] = data.to(device), labels.to(device),other[1].to(device).float().reshape(-1,1),other[2].to(device).float().reshape(-1,1)\n",
    "            logits = model(data,torch.cat((other[1],other[2]),dim=1))\n",
    "            loss = criterion(logits, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            top_p, top_class = logits.topk(1, dim=1)\n",
    "            y_pred.extend(top_class.data.cpu().numpy())\n",
    "            y_true.extend(labels.data.cpu().numpy())\n",
    "            equals = top_class == labels.reshape(top_class.shape)\n",
    "            test_acc += torch.sum(equals.type(torch.float)).detach().cpu()\n",
    "\n",
    "        test_acc/=len(valid_loader.dataset)\n",
    "        test_acc*=10\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=classes_cm, normalize='true')\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "        disp.plot()\n",
    "        plt.show()\n",
    "        print('Test accuracy : {}'.format(test_acc))\n",
    "    #loss 분석 코드\n",
    "    plt.plot(range(n_epochs),train_loss,label='train_loss')\n",
    "    plt.plot(range(n_epochs),valid_loss,label='valid_loss')\n",
    "    plt.xlabel('n_epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 살려 무조건 살려 그래야지 너가 살아"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-01T08:42:08.688663Z",
     "iopub.status.idle": "2023-06-01T08:42:08.689466Z",
     "shell.execute_reply": "2023-06-01T08:42:08.689241Z",
     "shell.execute_reply.started": "2023-06-01T08:42:08.689218Z"
    }
   },
   "outputs": [],
   "source": [
    "#모델 앙상블 위한 모델들 로드 과정\n",
    "model1 = VGG16()\n",
    "model2 = VGG16()\n",
    "model3 = VGG16()\n",
    "model4 = VGG16()\n",
    "model5 = VGG16()\n",
    "\n",
    "model1=torch.load(f'./model_1.pt')\n",
    "model2=torch.load(f'./model_2.pt')\n",
    "model3=torch.load(f'./model_3.pt')\n",
    "model4=torch.load(f'./model_4.pt')\n",
    "model5=torch.load(f'./model_5.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결과 CSV 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "model_list = [model1, model2, model3, model4, model5]  # K개의 모델\n",
    "predictions = []  # 예측 결과를 저장할 리스트\n",
    "id_list = []  # 파일 이름을 저장할 리스트\n",
    "\n",
    "with torch.no_grad():\n",
    "    for model in model_list:\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        model_predictions = []  # 모델의 예측 값을 저장할 리스트\n",
    "        id_list=[]\n",
    "        for data, file_name, other in test_loader:\n",
    "            data, other[0], other[1] = data.to(device), other[0].to(device).float().reshape(-1, 1), other[1].to(device).float().reshape(-1, 1)\n",
    "            logits = model(data, torch.cat((other[0], other[1]), dim=1))\n",
    "            softmax_probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "            model_predictions += softmax_probs.tolist()\n",
    "            id_list += file_name  # 파일 이름을 id_list에 추가\n",
    "        predictions.append(model_predictions)\n",
    "        \n",
    "# 소프트 보팅을 통해 최종 예측 결과 도출\n",
    "final_predictions = []\n",
    "for i in range(len(predictions[0])):\n",
    "    avg_probs = np.mean([pred[i] for pred in predictions], axis=0)\n",
    "    final_predictions.append(np.argmax(avg_probs))\n",
    "\n",
    "handout_result = pd.DataFrame({'Id': id_list, 'Category': final_predictions})\n",
    "handout_result.to_csv('./result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handout_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 00 10 20 30 40 50 60 70 80 9\n",
      "1 01 11 21 31 41 51 61 71 81 9\n",
      "2 02 12 22 32 42 52 62 72 82 9\n",
      "3 03 13 23 33 43 53 63 73 83 9\n",
      "4 04 14 24 34 44 54 64 74 84 9\n",
      "5 05 15 25 35 45 55 65 75 85 9\n",
      "6 06 16 26 36 46 56 66 76 86 9\n",
      "7 07 17 27 37 47 57 67 77 87 9\n",
      "8 08 18 28 38 48 58 68 78 88 9\n",
      "9 09 19 29 39 49 59 69 79 89 9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    for k in range(10):\n",
    "        print(i,k, end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-01T08:42:08.709678Z",
     "iopub.status.idle": "2023-06-01T08:42:08.710706Z",
     "shell.execute_reply": "2023-06-01T08:42:08.710466Z",
     "shell.execute_reply.started": "2023-06-01T08:42:08.710444Z"
    }
   },
   "source": [
    "### final_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고 자료"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
